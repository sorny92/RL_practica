%! Author = esteve
%! Date = 23/10/22

% Preamble
\documentclass[12pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=0.75in]{geometry}

\title{Práctica 1}
\author{Esteve Soria Fabián}
\counterwithin*{section}{part}
\begin{document}
    \maketitle

    \section*{Ejercicio 1}
    \subsection{¿Cuántas celdas/estados aparecen en el tablero?
    ¿Cuántas acciones puede ejecutar el agente?
    Si quisieras resolver el juego mediante aprendizaje por refuerzo, ¿cómo lo harías?}

    Aparecen 4x3 celdas, por tanto un total de 12 celdas o estados. \\
    El agente puede ejecutar 4 posibles acciones: mover arriba, derecha, abajo e izquierda.

    \textbf{STUFF MISSING HERE}
    
    \subsection{Abrir el fichero qlearningAgents.py y buscar la clase QLearningAgent. Describir los m´etodos que
    aparecen en ella.}
    
    \textbf{readQtable:} HAHAHAHAHAHA \\
    \textbf{writeQtable:} HAHAHAHAHAHA \\
    \textbf{computePosition:} HAHAHAHAHAHA \\
    \textbf{getQValue:} HAHAHAHAHAHA \\
    \textbf{computeValueFromQValues:} HAHAHAHAHAHA \\
    \textbf{computeActionFromQValues:} HAHAHAHAHAHA \\
    \textbf{getAction:} HAHAHAHAHAHA \\
    \textbf{update:} HAHAHAHAHAHA \\
    \textbf{getPolicy:} HAHAHAHAHAHA \\
    \textbf{getValue:} HAHAHAHAHAHA \\

    \subsection{Ejecuta ahora el agente anterior con:}
    \begin{verbatim}
    python gridworld.py -a q -k 100 -n 0
    \end{verbatim}
    \subsection{¿Qué información se muestra en el laberinto? ¿Qué aparece por terminal cuando se realizan los
    movimientos en el laberinto?}
    \subsection{¿Qué clase de movimiento realiza el agente anterior?}
    \subsection{¿Se pueden sacar varias políticas óptimas? Describe todas las políticas óptimas para este problema.}
    \subsection{Escribir el metodo update de la clase QLearningAgent utilizando las funciones de actualización del algoritmo
    Q-Learning. Para ello, inserta el código necesario allí donde aparezca la etiqueta INSERTA TU CODIGO
    AQUÍ siguiendo las instrucciones que se proporcionan, con el fin de conseguir el comportamiento deseado.}
    \subsection{Establece en el constructor de la clase QLearningAgent el valor de la variable epsilon a 0,05.
    Ejecuta nuevamente con:}
    \begin{verbatim}
    python gridworld.py -a q -k 100 -n 0
    \end{verbatim}
    \subsection*{¿Qué sucede?}
    \subsection{Después de la ejecución anterior, abrir el fichero qtable.txt. ¿Qué contiene?}


    \section*{Ejercicio 2}
    En el ejercicio anterior, siempre que el agente decid´ıa moverse hacia una direcci´on se mov´ıa en esa direcci´on
    con probabilidad 1. Es decir, se trataba de un MDP determinista. Ahora vamos a crear un MDP estoc´astico:
    \subsection{Ejecuta y juega un par de partidas con el agente manual:}
    \begin{verbatim}
    python gridworld.py -m -n 0.3
    \end{verbatim}
    \subsection*{¿Qué sucede? ¿Crees que el agente QLearningAgent seré capaz de aprender en este nuevo escenario?}
    \subsection{Reiniciar los valores de la tabla Q del fichero qtable.txt. Para ello ejecutar desde el terminal:}
    \begin{verbatim}
    cp qtable.ini.txt qtable.txt
    \end{verbatim}
    \subsection{Ejecutar el agente QLearningAgent:}
    \begin{verbatim}
    python gridworld.py -a q -k 100 -n 0.3
    \end{verbatim}
    \subsection{Tras unas cuantos episodios, ¿se genera la política óptima? Y si se genera, ¿se tarda más o menos
    que en el caso determinista?}

\end{document}